{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Vocabulary Object Segmentation using TAO Mask Grounding DINO\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n",
    "\n",
    "Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "<img align=\"center\" src=\"https://d29g4g2dyqv443.cloudfront.net/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png\" width=\"1080\">\n",
    "\n",
    "## What is Mask Grounding DINO?\n",
    "\n",
    "Built on top of [Grounding DINO](https://arxiv.org/abs/2303.05499), Mask Grounding DINO is a NVIDIA proprietary open-set object segmentation model. Mask Grounding DINO can detect and segment arbitrary objects with human inputs such as category names or referring expressions. Compared to Grounding DINO, Mask Grounding DINO has an additional mask branch and mask prediction head.\n",
    "\n",
    "In TAO, only single type of backbone network is supported: [Swin](https://arxiv.org/abs/2103.14030). In this notebook, we use the pretrained Swin-Tiny Grounding DINO and showcase how we can finetune on COCO dataset for the state of the art mAP result.\n",
    "\n",
    "### Sample prediction of Swin-Tiny + Mask Grounding DINO model\n",
    "<img align=\"center\" src=\"sample.jpg\" width=\"960\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "\n",
    "* Take a pretrained model and finetune a Mask Grounding DINO model on COCO dataset\n",
    "* Evaluate the trained model\n",
    "* Deploy the trained model\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "This notebook shows an example usecase of Mask Grounding DINO using Train Adapt Optimize (TAO) Toolkit.\n",
    "\n",
    "0. [Set up env variables and map drives](#head-0)\n",
    "1. [Installing the TAO launcher](#head-1)\n",
    "2. [Prepare dataset and pre-trained model](#head-2)\n",
    "3. [Provide training specification](#head-3)\n",
    "4. [Run TAO training](#head-4)\n",
    "5. [Evaluate a trained model](#head-5)\n",
    "6. [Visualize inferences](#head-6)\n",
    "7. [Deploy](#head-7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up env variables and map drives <a class=\"anchor\" id=\"head-0\"></a>\n",
    "\n",
    "The following notebook requires the user to set an env variable called the `$LOCAL_PROJECT_DIR` as the path to the users workspace. Please note that the dataset to run this notebook is expected to reside in the `$LOCAL_PROJECT_DIR/data`, while the TAO experiment generated collaterals will be output to `$LOCAL_PROJECT_DIR/grounding_dino/results`. More information on how to set up the dataset and the supported steps in the TAO workflow are provided in the subsequent cells.\n",
    "\n",
    "The TAO launcher uses docker containers under the hood, and **for our data and results directory to be visible to the docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the Environment Variables and amount of Shared Memory available to the TAO launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The code below creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results and cache. You should configure it for your specific case so these directories are correctly visible to the docker container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LOCAL_PROJECT_DIR=/home/ubuntu/jimmy/gitrepo/tao_tutorials/tao-experiments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Please define this local project directory that needs to be mapped to the TAO docker session.\n",
    "%env LOCAL_PROJECT_DIR=/home/ubuntu/jimmy/gitrepo/tao_tutorials/tao-experiments\n",
    "\n",
    "os.environ[\"HOST_DATA_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"data\")\n",
    "os.environ[\"HOST_RESULTS_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"mask_grounding_dino\", \"results\")\n",
    "\n",
    "# Set this path if you don't run the notebook from the samples directory.\n",
    "# %env NOTEBOOK_ROOT=~/tao-samples/mask_grounding_dino\n",
    "\n",
    "# The sample spec files are present in the same path as the downloaded samples.\n",
    "os.environ[\"HOST_SPECS_DIR\"] = os.path.join(\n",
    "    os.getenv(\"NOTEBOOK_ROOT\", os.getcwd()),\n",
    "    \"specs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $HOST_DATA_DIR\n",
    "!mkdir -p $HOST_SPECS_DIR\n",
    "!mkdir -p $HOST_RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "import os\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "tao_configs = {\n",
    "   \"Mounts\":[\n",
    "         # Mapping the Local project directory\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "            \"destination\": \"/workspace/tao-experiments\"\n",
    "        },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_DATA_DIR\"],\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_SPECS_DIR\"],\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_RESULTS_DIR\"],\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"~/.cache\",\n",
    "           \"destination\": \"/.cache\"\n",
    "       }\n",
    "   ],\n",
    "   \"DockerOptions\": {\n",
    "        \"shm_size\": \"64G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "         },\n",
    "        \"user\": \"{}:{}\".format(os.getuid(), os.getgid()),\n",
    "        \"network\": \"host\"\n",
    "   }\n",
    "}\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(tao_configs, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Mounts\": [\n",
      "        {\n",
      "            \"source\": \"/home/ubuntu/jimmy/gitrepo/tao_tutorials/tao-experiments\",\n",
      "            \"destination\": \"/workspace/tao-experiments\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"/home/ubuntu/jimmy/gitrepo/tao_tutorials/tao-experiments/data\",\n",
      "            \"destination\": \"/data\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"/home/ubuntu/jimmy/gitrepo/tao_tutorials/notebooks/tao_launcher_starter_kit/mask_grounding_dino/specs\",\n",
      "            \"destination\": \"/specs\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"/home/ubuntu/jimmy/gitrepo/tao_tutorials/tao-experiments/mask_grounding_dino/results\",\n",
      "            \"destination\": \"/results\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"~/.cache\",\n",
      "            \"destination\": \"/.cache\"\n",
      "        }\n",
      "    ],\n",
      "    \"DockerOptions\": {\n",
      "        \"shm_size\": \"64G\",\n",
      "        \"ulimits\": {\n",
      "            \"memlock\": -1,\n",
      "            \"stack\": 67108864\n",
      "        },\n",
      "        \"user\": \"1000:1000\",\n",
      "        \"network\": \"host\"\n",
      "    }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing the TAO launcher <a class=\"anchor\" id=\"head-1\"></a>\n",
    "The TAO launcher is a python package distributed as a python wheel listed in the `nvidia-pyindex` python index. You may install the launcher by executing the following cell.\n",
    "\n",
    "Please note that TAO Toolkit recommends users to run the TAO launcher in a virtual env with python 3.10. You may follow the instruction in this [page](https://virtualenvwrapper.readthedocs.io/en/latest/install.html) to set up a python virtual env using the `virtualenv` and `virtualenvwrapper` packages. Once you have setup virtualenvwrapper, please set the version of python to be used in the virtual env by using the `VIRTUALENVWRAPPER_PYTHON` variable. You may do so by running\n",
    "\n",
    "```sh\n",
    "export VIRTUALENVWRAPPER_PYTHON=/path/to/bin/python3.x\n",
    "```\n",
    "where x >= 6 and <= 8\n",
    "\n",
    "We recommend performing this step first and then launching the notebook from the virtual environment. In addition to installing TAO python package, please make sure of the following software requirements:\n",
    "* python >=3.7, <=3.10.x\n",
    "* docker-ce > 19.03.5\n",
    "* docker-API 1.40\n",
    "* nvidia-container-toolkit > 1.3.0-1\n",
    "* nvidia-container-runtime > 3.4.0-1\n",
    "* nvidia-docker2 > 2.5.0-1\n",
    "* nvidia-driver > 455+\n",
    "\n",
    "Once you have installed the pre-requisites, please log in to the docker registry nvcr.io by following the command below\n",
    "\n",
    "```sh\n",
    "docker login nvcr.io\n",
    "```\n",
    "\n",
    "You will be triggered to enter a username and password. The username is `$oauthtoken` and the password is the API key generated from `ngc.nvidia.com`. Please follow the instructions in the [NGC setup guide](https://docs.nvidia.com/ngc/ngc-overview/index.html#generating-api-key) to generate your own API key.\n",
    "\n",
    "Please note that TAO Toolkit recommends users to run the TAO launcher in a virtual env with python >=3.6.9. You may follow the instruction in this [page](https://virtualenvwrapper.readthedocs.io/en/latest/install.html) to set up a python virtual env using the virtualenv and virtualenvwrapper packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP this step IF you have already installed the TAO launcher.\n",
    "!pip3 install nvidia-tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration of the TAO Toolkit Instance\n",
      "task_group: ['model', 'dataset', 'deploy']\n",
      "format_version: 3.0\n",
      "toolkit_version: 5.5.0\n",
      "published_date: 08/26/2024\n"
     ]
    }
   ],
   "source": [
    "# View the versions of the TAO launcher\n",
    "!tao info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Prepare dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will be using the COCO dataset for the tutorial. The following script will download COCO dataset automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local dir\n",
    "!mkdir -p $HOST_DATA_DIR\n",
    "# Download the data\n",
    "!bash $HOST_SPECS_DIR/download_coco.sh $HOST_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 19931296\n",
      "drwxrwx--- 2 ubuntu ubuntu        4096 Oct 29 20:06 annotations\n",
      "-rw-rw---- 1 ubuntu ubuntu   252907541 Jul 10  2018 annotations_trainval2017.zip\n",
      "drwxrwx--- 2 ubuntu ubuntu        4096 Oct 29 20:36 tiny_val\n",
      "drwxrwxr-x 2 ubuntu ubuntu     4087808 Aug 31  2017 train2017\n",
      "-rw-rw---- 1 ubuntu ubuntu 19336861798 Jul 11  2018 train2017.zip\n",
      "drwxrwxr-x 2 ubuntu ubuntu      176128 Aug 31  2017 val2017\n",
      "-rw-rw---- 1 ubuntu ubuntu   815585330 Jul 11  2018 val2017.zip\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "!ls -l $HOST_DATA_DIR/raw-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DATA_DIR=/data\n",
      "env: SPECS_DIR=/specs\n",
      "env: RESULTS_DIR=/results\n"
     ]
    }
   ],
   "source": [
    "# Create ODVG folder\n",
    "!mkdir -p $HOST_DATA_DIR/odvg\n",
    "!mkdir -p $HOST_DATA_DIR/odvg/annotations\n",
    "\n",
    "# NOTE: The following paths are set from the perspective of the TAO Docker.\n",
    "\n",
    "# The data is saved here\n",
    "%env DATA_DIR = /data\n",
    "%env SPECS_DIR = /specs\n",
    "%env RESULTS_DIR = /results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert COCO to ODVG format required for Mask Grounding DINO\n",
    "!tao dataset annotations convert \\\n",
    "            -e $SPECS_DIR/convert.yaml \\\n",
    "            coco.ann_file=$DATA_DIR/raw-data/annotations/instances_train2017.json \\\n",
    "            results_dir=$DATA_DIR/odvg/annotations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 21:02:07,061 [TAO Toolkit] [INFO] root 160: Registry: ['nvcr.io']\n",
      "2024-11-15 21:02:07,152 [TAO Toolkit] [INFO] nvidia_tao_cli.components.instance_handler.local_instance 360: Running command in container: nvcr.io/nvidia/tao/tao-toolkit:5.5.0-data-services\n",
      "2024-11-15 21:02:07,358 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 301: Printing tty value True\n",
      "[2024-11-15 21:02:15,716 - TAO Toolkit - matplotlib - WARNING] Matplotlib created a temporary cache directory at /tmp/matplotlib-dghd700_ because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "[2024-11-15 21:02:16,194 - TAO Toolkit - matplotlib.font_manager - INFO] generated new fontManager\n",
      "sys:1: UserWarning: \n",
      "'convert.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "/usr/local/lib/python3.10/dist-packages/nvidia_tao_ds/core/hydra/hydra_runner.py:105: UserWarning: \n",
      "'convert.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  _run_hydra(\n",
      "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "/usr/local/lib/python3.10/dist-packages/nvidia_tao_ds/core/logging/logging.py:232: UserWarning: Log file already exists at /data/odvg/annotations/status.json\n",
      "  rank_zero_warn(\n",
      "Starting Data-services Annotation conversion.\n",
      "loading annotations into memory...\n",
      "Done (t=0.88s)\n",
      "creating index...\n",
      "index created!\n",
      "Processing detection annotations\n",
      "\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 3900.28it/s]ODVG annotation file is stored at /data/odvg/annotations/instances_val2017_odvg.jsonl\n",
      "Annotation conversion finished successfully.\n",
      "Sending telemetry data.\n",
      "[2024-11-15 21:02:29,664 - TAO Toolkit - root - INFO] ================> Start Reporting Telemetry <================\n",
      "[2024-11-15 21:02:29,664 - TAO Toolkit - root - INFO] Sending {'version': '5.5.0', 'action': 'convert', 'network': 'annotations', 'gpu': ['Tesla-V100-SXM2-16GB'], 'success': True, 'time_lapsed': 9} to https://api.tao.ngc.nvidia.com.\n",
      "[2024-11-15 21:02:30,242 - TAO Toolkit - root - INFO] Telemetry sent successfully.\n",
      "[2024-11-15 21:02:30,242 - TAO Toolkit - root - INFO] ================> End Reporting Telemetry <================\n",
      "Execution status: PASS\n",
      "2024-11-15 21:02:30,980 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 363: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# Convert COCO to ODVG format required for Mask Grounding DINO\n",
    "!tao dataset annotations convert \\\n",
    "            -e $SPECS_DIR/convert.yaml \\\n",
    "            coco.ann_file=$DATA_DIR/raw-data/annotations/instances_val2017.json \\\n",
    "            results_dir=$DATA_DIR/odvg/annotations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert COCO validation annotations to have categoy id ranging from 0 to 79.\n",
    "# This is required for computing validation loss during Mask Grounding DINO training.\n",
    "!tao dataset annotations convert \\\n",
    "            -e $SPECS_DIR/convert.yaml \\\n",
    "            coco.ann_file=$DATA_DIR/raw-data/annotations/instances_val2017.json \\\n",
    "            results_dir=$DATA_DIR/odvg/annotations/ \\\n",
    "            data.output_format=\"COCO\" \\\n",
    "            coco.use_all_categories=True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Download pre-trained model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use NGC CLI to get the pre-trained models. For more details, go to [ngc.nvidia.com](ngc.nvidia.com) and click the SETUP on the navigation bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing NGC CLI on the local machine.\n",
    "## Download and install\n",
    "%env CLI=ngccli_cat_linux.zip\n",
    "!mkdir -p $LOCAL_PROJECT_DIR/ngccli\n",
    "\n",
    "# Remove any previously existing CLI installations\n",
    "!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n",
    "!unzip -u \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n",
    "!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model list nvidia/tao/grounding_dino:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull pretrained model from NGC\n",
    "!ngc registry model download-version nvidia/tao/grounding_dino:grounding_dino_swin_tiny_commercial_trainable_v1.0 --dest $LOCAL_PROJECT_DIR/mask_grounding_dino/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check that model is downloaded into dir.\n",
      "total 2022184\n",
      "-rw-r--r-- 1 ubuntu ubuntu       2360 Oct 29 20:23 experiment.yaml\n",
      "-rw-r--r-- 1 ubuntu ubuntu 2070704394 Oct 29 20:24 grounding_dino_swin_tiny_commercial_trainable.pth\n"
     ]
    }
   ],
   "source": [
    "print(\"Check that model is downloaded into dir.\")\n",
    "!ls -l $LOCAL_PROJECT_DIR/mask_grounding_dino/grounding_dino_vgrounding_dino_swin_tiny_commercial_trainable_v1.0/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Provide training specification <a class=\"anchor\" id=\"head-3\"></a>\n",
    "\n",
    "We provide specification files to configure the training parameters including:\n",
    "\n",
    "* dataset: configure the dataset and augmentation methods\n",
    "    * train_data_sources:\n",
    "        * image_dir: the root directory for train images\n",
    "        * json_file: ODVG annotation file\n",
    "        * label_map: category id and category mapping\n",
    "    * val_data_sources: \n",
    "        * image_dir: the root directory for validation images\n",
    "        * json_file: annotation file for validation data. Required to be in COCO json format and the categoy id should be in the range of 0 ~ # of classes - 1\n",
    "    * max_labels: max number of positive + negative labels seen in a single batch. Larger max_labels usually result in better accuracy with longer training time.\n",
    "    * batch_size: batch size for dataloader\n",
    "    * workers: number of workers to do data loading\n",
    "* model: configure the model setting\n",
    "    * pretrained_backbone_path: path to the pretrained backbone model. Only Swin-variants are supported\n",
    "    * num_feature_levels: number of feature levels used from backbone\n",
    "    * dec_layers: number of decoder layers\n",
    "    * enc_layers: number of encoder layers\n",
    "    * num_queries: number of queries for the model\n",
    "    * num_select: number of top-k proposals to select from\n",
    "    * use_dn: flag to enable denoising during training\n",
    "    * dropout_ratio: drop out ratio\n",
    "* train: configure the training hyperparameters\n",
    "    * num_gpus: number of gpus \n",
    "    * num_nodes: number of nodes (num_nodes=1 for single node)\n",
    "    * val_interval: validation interval\n",
    "    * optim:\n",
    "        * lr_backbone: learning rate for backbone\n",
    "        * lr: learning rate for the rest of the model\n",
    "        * lr_steps: learning rate decay step milestone (MultiStep)\n",
    "    * num_epochs: number of epochs\n",
    "    * activation_checkpoint: recompute activations in the backward to save GPU memory. Default is `True`.\n",
    "    * precision: If set to fp16/bf16, the training is run on Automatic Mixed Precision (AMP)\n",
    "    * distributed_strategy: Default is `ddp`. `ddp_sharded` is also supported.\n",
    "\n",
    "Please refer to the TAO documentation about Grounding DINO to get all the parameters that are configurable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "  num_gpus: 1\n",
      "  num_nodes: 1\n",
      "  validation_interval: 1\n",
      "  optim:\n",
      "    lr_backbone: 2e-05\n",
      "    lr: 0.0002\n",
      "    lr_steps: [10, 20]\n",
      "    momentum: 0.9\n",
      "  num_epochs: 1\n",
      "  freeze: [\"backbone\", \"bert\"]\n",
      "  pretrained_model_path: /workspace/tao-experiments/mask_grounding_dino/grounding_dino_vgrounding_dino_swin_tiny_commercial_trainable_v1.0/grounding_dino_swin_tiny_commercial_trainable.pth\n",
      "  precision: bf16\n",
      "  activation_checkpoint: True\n",
      "dataset:\n",
      "  train_data_sources:\n",
      "    - image_dir: /data/raw-data/train2017/\n",
      "      json_file: /data/odvg/annotations/instances_train2017_odvg.jsonl\n",
      "      label_map: /data/odvg/annotations/instances_train2017_odvg_labelmap.json\n",
      "  val_data_sources:\n",
      "    image_dir: /data/raw-data/val2017/\n",
      "    json_file: /data/odvg/annotations/instances_val2017_remapped.json\n",
      "  max_labels: 80\n",
      "  batch_size: 4\n",
      "  workers: 8\n",
      "model:\n",
      "  backbone: swin_tiny_224_1k\n",
      "  num_feature_levels: 4\n",
      "  dec_layers: 6\n",
      "  enc_layers: 6\n",
      "  num_queries: 900\n",
      "  dropout_ratio: 0.0\n",
      "  dim_feedforward: 2048\n",
      "  loss_types: ['labels', 'boxes', 'masks']\n",
      "  log_scale: auto\n",
      "  class_embed_bias: True\n"
     ]
    }
   ],
   "source": [
    "!cat $HOST_SPECS_DIR/train.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "  num_gpus: 1\n",
      "  num_nodes: 1\n",
      "  validation_interval: 1\n",
      "  optim:\n",
      "    lr_backbone: 2e-05\n",
      "    lr: 0.0002\n",
      "    lr_steps: [10, 20]\n",
      "    momentum: 0.9\n",
      "  num_epochs: 1\n",
      "  freeze: [\"backbone\", \"bert\"]\n",
      "  pretrained_model_path: /workspace/tao-experiments/mask_grounding_dino/grounding_dino_vgrounding_dino_swin_tiny_commercial_trainable_v1.0/grounding_dino_swin_tiny_commercial_trainable.pth\n",
      "  precision: bf16\n",
      "  activation_checkpoint: True\n",
      "dataset:\n",
      "  train_data_sources:\n",
      "    - image_dir: /data/raw-data/val2017/\n",
      "      json_file: /data/odvg/annotations/instances_val2017_odvg.jsonl\n",
      "      label_map: /data/odvg/annotations/instances_val2017_odvg_labelmap.json\n",
      "  val_data_sources:\n",
      "    image_dir: /data/raw-data/val2017/\n",
      "    json_file: /data/odvg/annotations/instances_val2017_remapped.json\n",
      "  max_labels: 80\n",
      "  batch_size: 4\n",
      "  workers: 8\n",
      "model:\n",
      "  backbone: swin_tiny_224_1k\n",
      "  num_feature_levels: 4\n",
      "  dec_layers: 6\n",
      "  enc_layers: 6\n",
      "  num_queries: 900\n",
      "  dropout_ratio: 0.0\n",
      "  dim_feedforward: 2048\n",
      "  loss_types: ['labels', 'boxes', 'masks']\n",
      "  log_scale: auto\n",
      "  class_embed_bias: True\n"
     ]
    }
   ],
   "source": [
    "!cat $HOST_SPECS_DIR/train_quick_test.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run TAO training <a class=\"anchor\" id=\"head-4\"></a>\n",
    "* Provide the sample spec file and the output directory location for models\n",
    "* Evaluation uses COCO metrics. For more info, please refer to: https://cocodataset.org/#detection-eval\n",
    "* *WARNING*: [according to the orirginal paper](https://arxiv.org/abs/2303.05499), COCO training was conducted using 8 A100 gpus. As a result, **we highly recommend that you run training with multiple high-end gpus (e.g. V100, A100)**\n",
    "* *Note*: The current training config should fit on a GPU with 16G of memory. Try to lower `batch_size` if you face OOM issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"For multi-GPU, change num_gpus in train.yaml based on your machine or update the NUM_TRAIN_GPUS env variable in the line below to the desired number of GPUs.\")\n",
    "os.environ[\"NUM_TRAIN_GPUS\"] = \"1\"\n",
    "\n",
    "!tao model mask_grounding_dino train -e $SPECS_DIR/train.yaml train.num_gpus=$NUM_TRAIN_GPUS results_dir=$RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For multi-GPU, change num_gpus in train.yaml based on your machine or update the NUM_TRAIN_GPUS env variable in the line below to the desired number of GPUs.\n",
      "2024-11-16 02:35:41,686 [TAO Toolkit] [INFO] root 160: Registry: ['nvcr.io']\n",
      "2024-11-16 02:35:41,767 [TAO Toolkit] [INFO] nvidia_tao_cli.components.instance_handler.local_instance 360: Running command in container: nvcr.io/nvidia/tao/tao-toolkit:5.5.0-pyt\n",
      "2024-11-16 02:35:41,827 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 301: Printing tty value True\n",
      "sys:1: UserWarning: \n",
      "'train_quick_test.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "/usr/local/lib/python3.10/dist-packages/nvidia_tao_pytorch/core/hydra/hydra_runner.py:107: UserWarning: \n",
      "'train_quick_test.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  _run_hydra(\n",
      "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "/usr/local/lib/python3.10/dist-packages/nvidia_tao_pytorch/core/loggers/api_logging.py:236: UserWarning: Log file already exists at /results/train/status.json\n",
      "  rank_zero_warn(\n",
      "Seed set to 1234\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3553.)\n",
      "return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]Train results will be saved at: /results/train\n",
      "final text_encoder_type: bert-base-uncased\n",
      "\n",
      "tokenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<00:00, 326kB/s]\n",
      "config.json: 100%|██████████| 570/570 [00:00<00:00, 4.25MB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 40.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 54.2MB/s]\n",
      "model.safetensors: 100%|██████████| 440M/440M [00:05<00:00, 75.0MB/s]]final text_encoder_type: bert-base-uncased\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /results/train exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | model          | MaskGDINOModel   | 174 M \n",
      "1 | matcher        | HungarianMatcher | 0     \n",
      "2 | criterion      | SetCriterion     | 0     \n",
      "3 | box_processors | PostProcess      | 0     \n",
      "----------------------------------------------------\n",
      "37.9 M    Trainable params\n",
      "137 M     Non-trainable params\n",
      "174 M     Total params\n",
      "699.748   Total estimated model params size (MB)\n",
      "skip layer model.controller.layers.0.bias as it doesn't exist in the checkpoint\n",
      "skip layer model.controller.layers.0.weight as it doesn't exist in the checkpoint\n",
      "skip layer model.controller.layers.1.bias as it doesn't exist in the checkpoint\n",
      "skip layer model.controller.layers.1.weight as it doesn't exist in the checkpoint\n",
      "skip layer model.controller.layers.2.bias as it doesn't exist in the checkpoint\n",
      "skip layer model.controller.layers.2.weight as it doesn't exist in the checkpoint\n",
      "skip layer model.mask_head.jia_dcn.bias as it doesn't exist in the checkpoint\n",
      "skip layer model.mask_head.jia_dcn.weight as it doesn't exist in the checkpoint\n",
      "skip layer model.mask_head.lay1.bias as it doesn't exist in the checkpoint\n",
      "skip layer model.mask_head.lay1.weight as it doesn't exist in the checkpoint\n",
      "skip layer model.mask_head.lay2.bias as it doesn't exist in the checkpoint\n",
      "skip layer model.mask_head.lay2.weight as it doesn't exist in the checkpoint\n",
      "skip layer model.mask_head.lay3.bias as it doesn't exist in the checkpoint\n",
      "skip layer model.mask_head.lay3.weight as it doesn't exist in the checkpoint\n",
      "skip layer model.mask_head.lay4.bias as it doesn't exist in the checkpoint\n",
      "skip layer model.mask_head.lay4.weight as it doesn't exist in the checkpoint\n",
      "\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:993: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:11<00:00,  0.17it/s] Validation mAP (bbox): 0.5223202554545928\n",
      "\n",
      "\n",
      " Validation mAP50 (bbox): 0.6954759268422085\n",
      "\n",
      "\n",
      " Validation mAP (segm): 0.011076179360233173\n",
      "\n",
      "\n",
      " Validation mAP50 (segm): 0.027286920999868375\n",
      "\n",
      "\n",
      "                                                                           \n",
      "Serializing 4952 elements to byte tensors and concatenating them all ...\n",
      "Serialized dataset takes 24.77 MiB\n",
      "\n",
      "Epoch 0: 100%|██████████| 1238/1238 [2:04:53<00:00,  0.17it/s, v_num=1, train_loss_step=24.80, train_dice_loss_step=0.339, train_mask_loss_step=0.0341, train_lr=2e-5]]]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1250 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1250 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 1/1250 [00:05<1:52:29,  0.19it/s]\u001b[AError executing job with overrides: ['train.num_gpus=1', 'results_dir=/results']Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia_tao_pytorch/core/decorators/workflow.py\", line 69, in _func\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia_tao_pytorch/core/decorators/workflow.py\", line 48, in _func\n",
      "    runner(cfg, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia_tao_pytorch/cv/mask_grounding_dino/scripts/train.py\", line 148, in main\n",
      "    run_experiment(experiment_config=cfg)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia_tao_pytorch/cv/mask_grounding_dino/scripts/train.py\", line 134, in run_experiment\n",
      "    trainer.fit(pt_model, dm, ckpt_path=resume_ckpt)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 543, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 579, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 986, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1032, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 139, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 287, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 182, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 135, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 396, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 309, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia_tao_pytorch/cv/mask_grounding_dino/model/pl_gdino_model.py\", line 297, in validation_step\n",
      "    outputs = self.model(data,\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1536, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia_tao_pytorch/cv/mask_grounding_dino/model/build_nn_model.py\", line 205, in forward\n",
      "    x = self.model(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1536, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia_tao_pytorch/cv/mask_grounding_dino/model/mask_groundingdino.py\", line 336, in forward\n",
      "    outputs = self.forward_mask_head(outputs, memory, spatial_shapes,\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia_tao_pytorch/cv/mask_grounding_dino/model/mask_groundingdino.py\", line 531, in forward_mask_head\n",
      "    mask_logits = self.dynamic_mask_with_coords(decod_feat_f, reference_points, mask_head_params,\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia_tao_pytorch/cv/mask_grounding_dino/model/mask_groundingdino.py\", line 475, in dynamic_mask_with_coords\n",
      "mask_logits = self.mask_heads_forward(mask_head_inputs, weights, biases, mask_head_params.shape[0])  File \"/usr/local/lib/python3.10/dist-packages/nvidia_tao_pytorch/cv/mask_grounding_dino/model/mask_groundingdino.py\", line 389, in mask_heads_forward\n",
      "    x = F.conv2d(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU \n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
      "\n",
      "Epoch 0: 100%|██████████| 1238/1238 [2:05:04<00:00,  0.16it/s, v_num=1, train_loss_step=24.80, train_dice_loss_step=0.339, train_mask_loss_step=0.0341, train_lr=2e-5]\n",
      "\n",
      "                                                                           \u001b[A[2024-11-16 04:41:30,207 - TAO Toolkit - root - INFO] Sending telemetry data.\n",
      "[2024-11-16 04:41:30,207 - TAO Toolkit - root - INFO] ================> Start Reporting Telemetry <================\n",
      "[2024-11-16 04:41:30,207 - TAO Toolkit - root - INFO] Sending {'version': '5.5.0', 'action': 'train', 'network': 'mask_grounding_dino', 'gpu': ['Tesla-V100-SXM2-16GB'], 'success': False, 'time_lapsed': 7541} to https://api.tao.ngc.nvidia.com.\n",
      "[2024-11-16 04:41:30,787 - TAO Toolkit - root - INFO] Telemetry sent successfully.\n",
      "[2024-11-16 04:41:30,787 - TAO Toolkit - root - INFO] ================> End Reporting Telemetry <================\n",
      "[2024-11-16 04:41:30,788 - TAO Toolkit - root - WARNING] Execution status: FAIL\n",
      "2024-11-16 04:41:31,624 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 363: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "print(\"For multi-GPU, change num_gpus in train.yaml based on your machine or update the NUM_TRAIN_GPUS env variable in the line below to the desired number of GPUs.\")\n",
    "os.environ[\"NUM_TRAIN_GPUS\"] = \"1\"\n",
    "\n",
    "!tao model mask_grounding_dino train -e $SPECS_DIR/train_quick_test.yaml train.num_gpus=$NUM_TRAIN_GPUS results_dir=$RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/results\n"
     ]
    }
   ],
   "source": [
    "!echo $RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained checkpoints:\n",
      "---------------------\n",
      "total 16K\n",
      "drwxr-xr-x 3 ubuntu ubuntu 4.0K Nov 12 19:45 lightning_logs\n",
      "-rw-r--r-- 1 ubuntu ubuntu 4.2K Nov 16 02:35 experiment.yaml\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.6K Nov 16 04:41 status.json\n"
     ]
    }
   ],
   "source": [
    "print('Trained checkpoints:')\n",
    "print('---------------------')\n",
    "!ls -ltrh $HOST_RESULTS_DIR/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set NUM_EPOCH to the epoch corresponding to any saved checkpoint\n",
    "# %env NUM_EPOCH=029\n",
    "\n",
    "# Get the name of the checkpoint corresponding to your set epoch\n",
    "# tmp=!ls $HOST_RESULTS_DIR/train/*.pth | grep epoch_$NUM_EPOCH\n",
    "# %env CHECKPOINT={tmp[0]}\n",
    "\n",
    "# Or get the latest checkpoint\n",
    "os.environ[\"CHECKPOINT\"] = os.path.join(os.getenv(\"HOST_RESULTS_DIR\"), \"train/mask_gdino_model_latest.pth\")\n",
    "\n",
    "print('Rename a trained model: ')\n",
    "print('---------------------')\n",
    "!cp $CHECKPOINT $HOST_RESULTS_DIR/train/mask_grounding_dino_model.pth\n",
    "!ls -ltrh $HOST_RESULTS_DIR/train/mask_grounding_dino_model.pth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate a trained model <a class=\"anchor\" id=\"head-5\"></a>\n",
    "\n",
    "In this section, we run the `evaluate` tool to evaluate the trained model and produce the mAP metric.\n",
    "\n",
    "We provide evaluate.yaml specification files to configure the evaluate parameters including:\n",
    "\n",
    "* model: configure the model setting\n",
    "    * this config should remain same as your trained model's configuration.\n",
    "* dataset: configure the dataset and augmentation methods\n",
    "    * test_data_sources:\n",
    "        * image_dir: the root directory for evaluatation images    \n",
    "        * json_file: Required to be in COCO json format and the categoy id should be in the range of 0 ~ # of classes - 1\n",
    "    * batch_size\n",
    "    * workers\n",
    "* evaluate:\n",
    "    * num_gpus: number of gpus\n",
    "    * conf_threshold: a threshold for confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on TAO model\n",
    "!tao model mask_grounding_dino evaluate \\\n",
    "            -e $SPECS_DIR/evaluate.yaml \\\n",
    "            evaluate.checkpoint=$RESULTS_DIR/train/mask_grounding_dino_model.pth \\\n",
    "            results_dir=$RESULTS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Inferences <a class=\"anchor\" id=\"head-6\"></a>\n",
    "In this section, we run the `inference` tool to generate inferences on the trained models and visualize the results. The `inference` tool produces annotated image outputs and txt files that contain prediction information.\n",
    "\n",
    "We provide inference.yaml specification files to configure the inference parameters including:\n",
    "\n",
    "* model: configure the model setting\n",
    "    * this config should remain same as your trained model's configuration\n",
    "* dataset: configure the dataset and augmentation methods\n",
    "    * infer_data_sources:\n",
    "        * image_dir: the list of directories for inference images\n",
    "        * captions: list of phrases to run inference on. E.g. [\"person\", \"black cat\"]\n",
    "    * batch_size\n",
    "    * workers\n",
    "* inference\n",
    "    * conf_threshold: the confidence score threshold\n",
    "    * color_map: the color mapping for each phrase. The predicted bbox will be drawn with mapped color for each phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model mask_grounding_dino inference \\\n",
    "        -e $SPECS_DIR/infer.yaml \\\n",
    "        inference.checkpoint=$RESULTS_DIR/train/mask_grounding_dino_model.pth \\\n",
    "        results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid visualizer\n",
    "!pip3 install \"matplotlib>=3.3.3, <4.0\"\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from math import ceil\n",
    "valid_image_ext = ['.jpg']\n",
    "\n",
    "def visualize_images(output_path, num_cols=4, num_images=10):\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images.\n",
    "IMAGE_DIR = os.path.join(os.environ['HOST_RESULTS_DIR'], \"inference\", \"images_annotated\")\n",
    "COLS = 2 # number of columns in the visualizer grid.\n",
    "IMAGES = 4 # number of images to visualize.\n",
    "\n",
    "visualize_images(IMAGE_DIR, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deploy (**experimental**) <a class=\"anchor\" id=\"head-7\"></a>\n",
    "\n",
    "**Note:** The batch size of the exported ONNX model and TensorRT must be 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to ONNX model\n",
    "!tao model mask_grounding_dino export \\\n",
    "           -e $SPECS_DIR/export.yaml \\\n",
    "           export.checkpoint=$RESULTS_DIR/train/mask_grounding_dino_model.pth \\\n",
    "           export.onnx_file=$RESULTS_DIR/export/mask_grounding_dino_model.onnx \\\n",
    "           results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate TensorRT engine using tao deploy\n",
    "!tao deploy mask_grounding_dino gen_trt_engine -e $SPECS_DIR/gen_trt_engine.yaml \\\n",
    "                               gen_trt_engine.onnx_file=$RESULTS_DIR/export/mask_grounding_dino_model.onnx \\\n",
    "                               gen_trt_engine.trt_engine=$RESULTS_DIR/gen_trt_engine/mask_grounding_dino_model.engine \\\n",
    "                               results_dir=$RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with generated TensorRT engine\n",
    "!tao deploy mask_grounding_dino inference -e $SPECS_DIR/infer.yaml \\\n",
    "                              inference.trt_engine=$RESULTS_DIR/gen_trt_engine/mask_grounding_dino_model.engine \\\n",
    "                              results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images.\n",
    "IMAGE_DIR = os.path.join(os.environ['HOST_RESULTS_DIR'], \"trt_inference\", \"images_annotated\")\n",
    "COLS = 2 # number of columns in the visualizer grid.\n",
    "IMAGES = 4 # number of images to visualize.\n",
    "\n",
    "visualize_images(IMAGE_DIR, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has come to an end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
